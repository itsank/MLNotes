# General Machine Learning

## Gradient Descent

### The Zen of gradient descent [link](http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html)
+ Basic gradient descent is robust to noise
+ Accelerated gradient descent is much more brittle
+ Small improvements in running time don’t trump major losses in robustness.
+ “which algorithm is better” is rarely answered by looking at a single proxy such as “convergence rate”
+ Basic Gradient Descent
  + minimizeses a convex function (twice differentiable and strongly convex)
  + Let $n>2$ be an integer.
Consider the equation:
\[
a^n + b^n = c^n
\]

## Siasmese Network
+ Triplet Loss(http://datahacker.rs/032-cnn-triplet-loss/)
